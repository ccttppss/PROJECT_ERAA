"""
ì‹¤í—˜ 1/4: ABSA ë²¤ì¹˜ë§ˆí¬ ë° Ablation Study - SemEval 2014 ë°ì´í„°ì…‹ ì‚¬ìš©

ëª©ì : 
1. SemEval 2014 í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ LLM vs ë³„ì  ê¸°ë°˜ ë¹„êµ
2. ê° ì»´í¬ë„ŒíŠ¸ ê¸°ì—¬ë„ ë¶„ì„ (Ablation)

ë°ì´í„°: SemEval 2014 Task 4 - Aspect Based Sentiment Analysis
- Laptop: 654 aspects (test)
- Restaurant: 1,134 aspects (test)
"""
import sys
import json
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Any, Tuple
import random
import time

# src í´ë”ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))

from agents.sentiment_agent import SentimentAnalysisAgent
from services.llm_service import create_llm_service
from utils.logger import get_logger
import yaml
from jinja2 import Template

logger = get_logger(__name__)


def load_llm_service():
    """LLM ì„œë¹„ìŠ¤ ë¡œë“œ"""
    config_path = project_root / 'src' / 'config' / 'llm_config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        llm_config = yaml.safe_load(f)
    return create_llm_service(llm_config)


def load_semeval_dataset(domain: str = 'laptop', split: str = 'test') -> List[Dict]:
    """
    SemEval 2014 ë°ì´í„°ì…‹ ë¡œë“œ
    
    Args:
        domain: 'laptop' or 'restaurant'
        split: 'train' or 'test'
    
    Returns:
        List of {text, aspects: [{term, polarity, from, to}]}
    """
    filename = f"{domain}_{split}.xml"
    semeval_path = project_root / 'datasets' / 'semeval2014' / filename
    
    if not semeval_path.exists():
        raise FileNotFoundError(f"SemEval ë°ì´í„°ì…‹ ì—†ìŒ: {semeval_path}")
    
    tree = ET.parse(semeval_path)
    root = tree.getroot()
    
    samples = []
    for sentence in root.findall('.//sentence'):
        text_elem = sentence.find('text')
        if text_elem is None:
            continue
        
        text = text_elem.text
        aspects = []
        
        for aspect_term in sentence.findall('.//aspectTerm'):
            term = aspect_term.get('term')
            polarity = aspect_term.get('polarity')
            from_idx = aspect_term.get('from')
            to_idx = aspect_term.get('to')
            
            # conflictì™€ unknownì€ ì œì™¸
            if polarity in ['positive', 'negative', 'neutral']:
                aspects.append({
                    'term': term,
                    'polarity': polarity,
                    'from': int(from_idx) if from_idx else 0,
                    'to': int(to_idx) if to_idx else 0
                })
        
        if aspects:
            samples.append({
                'text': text,
                'aspects': aspects
            })
    
    return samples


def evaluate_rating_based(samples: List[Dict]) -> Dict:
    """
    ë³„ì  ê¸°ë°˜ í‰ê°€ (ëª¨ë“  aspectì— ë™ì¼ ê°ì •)
    
    ê°€ì •: ë³„ì  ì—†ì´ ì „ì²´ ë¦¬ë·° ê¸°ì¤€ìœ¼ë¡œ majority ê°ì • ì˜ˆì¸¡
    """
    total_aspects = 0
    correct_aspects = 0
    
    # Baseline: í†µê³„ì ìœ¼ë¡œ ê°€ì¥ ë§ì€ 'positive'ë¡œ ëª¨ë‘ ì˜ˆì¸¡ (Majority Class)
    majority = 'positive'
    
    for sample in samples:
        for aspect in sample['aspects']:
            total_aspects += 1
            if majority == aspect['polarity']:
                correct_aspects += 1
    
    return {
        'method': 'rating_based',
        'accuracy': correct_aspects / total_aspects if total_aspects > 0 else 0,
        'correct': correct_aspects,
        'total': total_aspects
    }


def evaluate_llm_based(samples: List[Dict], agent: SentimentAnalysisAgent,
                       max_samples: int = 100) -> Dict:
    """LLM ê¸°ë°˜ aspect sentiment ë¶„ë¥˜"""
    if not agent.llm_service:
        return {'method': 'llm_based', 'accuracy': 0, 'correct': 0, 'total': 0}
    
    eval_samples = samples[:max_samples] if len(samples) > max_samples else samples
    
    total_aspects = 0
    correct_aspects = 0
    details = []
    
    for idx, sample in enumerate(eval_samples):
        text = sample['text']
        true_aspects = sample['aspects']
        
        # ì‹¤ì œ aspect term ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        aspect_terms = [a['term'] for a in true_aspects]
        
        try:
            sentiment_result = agent._llm_aspect_sentiment(text, aspect_terms)
            time.sleep(2.0)  # Ollama ê³¼ë¶€í•˜ ë°©ì§€
        except Exception as e:
            logger.warning(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            sentiment_result = {}
            time.sleep(5.0)
        
        for aspect in true_aspects:
            term = aspect['term'].lower()
            true_polarity = aspect['polarity']
            total_aspects += 1
            
            predicted = sentiment_result.get(term, 'not_found')
            if predicted in ['not_mentioned', 'unknown']:
                predicted = 'not_found'
            
            if predicted == true_polarity:
                correct_aspects += 1
        
        if (idx + 1) % 20 == 0:
            print(f"   ì§„í–‰: {idx + 1}/{len(eval_samples)}")
    
    return {
        'method': 'llm_based',
        'accuracy': correct_aspects / total_aspects if total_aspects > 0 else 0,
        'correct': correct_aspects,
        'total': total_aspects
    }


def evaluate_without_keyword(samples: List[Dict], llm_service,
                             max_samples: int = 100) -> Dict:
    """í‚¤ì›Œë“œ ì¶”ì¶œ ì—†ì´ ì „ì²´ ë¦¬ë·° â†’ LLM"""
    if not llm_service:
        return {'method': 'without_keyword', 'accuracy': 0, 'correct': 0, 'total': 0}
    
    eval_samples = samples[:max_samples]
    
    simple_template = Template("""
ë‹¤ìŒ ë¦¬ë·°ì—ì„œ ì£¼ì–´ì§„ aspect termì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.

ë¦¬ë·°: "{{ text }}"
Aspect: "{{ aspect }}"

positive, negative, neutral ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
ë°˜ë“œì‹œ JSON í˜•ì‹: {"sentiment": "positive|negative|neutral"}
""")
    
    total = 0
    correct = 0
    
    for idx, sample in enumerate(eval_samples):
        for aspect in sample['aspects']:
            try:
                prompt = simple_template.render(
                    text=sample['text'][:300],
                    aspect=aspect['term']
                )
                response = llm_service.generate_json(prompt, max_tokens=10000, temperature=0.3)
                time.sleep(1.0) # aspect ë‹¨ìœ„ í˜¸ì¶œì´ë¯€ë¡œ ì§§ê²Œ
                
                if response and 'sentiment' in response:
                    if response['sentiment'] == aspect['polarity']:
                        correct += 1
            except:
                pass
            total += 1
        
        if (idx + 1) % 20 == 0:
            print(f"   ì§„í–‰: {idx + 1}/{len(eval_samples)}")
    
    return {
        'method': 'without_keyword',
        'accuracy': correct / total if total > 0 else 0,
        'correct': correct,
        'total': total
    }


def run_semeval_experiment(domain: str = 'laptop', max_samples: int = 100):
    """SemEval 2014 ABSA ì‹¤í—˜ ì‹¤í–‰"""
    print("=" * 70)
    print(f"ğŸ”¬ ì‹¤í—˜ 1: ABSA ë²¤ì¹˜ë§ˆí¬ (SemEval 2014 - {domain.capitalize()})")
    print("=" * 70)
    print("ğŸ“š ë°ì´í„°: SemEval 2014 Task 4")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“‚ SemEval 2014 {domain} ë°ì´í„°ì…‹ ë¡œë“œ...")
    try:
        samples = load_semeval_dataset(domain, 'test')
        total_aspects = sum(len(s['aspects']) for s in samples)
        
        print(f"   âœ… ë¬¸ì¥ ìˆ˜: {len(samples)}")
        print(f"   âœ… Aspect ìˆ˜: {total_aspects}")
    except FileNotFoundError as e:
        print(f"   âŒ {e}")
        return None
    
    # LLM ì„œë¹„ìŠ¤ ë¡œë“œ
    print("\nğŸ“¡ LLM ì„œë¹„ìŠ¤ ë¡œë“œ...")
    try:
        llm_service = load_llm_service()
        print("   âœ… LLM service ready")
    except Exception as e:
        print(f"   âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        llm_service = None
    
    config = {'log_level': 'WARNING'}
    agent = SentimentAnalysisAgent(config, llm_service)
    
    results = {}
    
    # ===== ë°©ë²• A: ë³„ì /ë‹¤ìˆ˜ê²° ê¸°ë°˜ =====
    print("\n" + "-" * 50)
    print("ğŸ“Œ ì¡°ê±´ A: ë³„ì  ê¸°ë°˜ (ëª¨ë“  aspect ë™ì¼ ê°ì •)")
    
    result_rating = evaluate_rating_based(samples)
    results['rating_based'] = result_rating
    
    print(f"   Accuracy: {result_rating['accuracy']:.2%}")
    print(f"   ì •ë‹µ: {result_rating['correct']}/{result_rating['total']}")
    
    # ===== ë°©ë²• B: Full Model (LLM + í‚¤ì›Œë“œ) =====
    print("\n" + "-" * 50)
    print(f"ğŸ“Œ ì¡°ê±´ B: Full Model - LLM + í‚¤ì›Œë“œ ({max_samples}ê°œ ìƒ˜í”Œ)")
    
    if llm_service:
        result_llm = evaluate_llm_based(samples, agent, max_samples)
        results['full_model'] = result_llm
        
        print(f"\n   Accuracy: {result_llm['accuracy']:.2%}")
        print(f"   ì •ë‹µ: {result_llm['correct']}/{result_llm['total']}")
    else:
        print("   âš ï¸ LLM ì„œë¹„ìŠ¤ ì—†ìŒ")
    
    # ===== ë°©ë²• C: w/o í‚¤ì›Œë“œ ì¶”ì¶œ =====
    print("\n" + "-" * 50)
    print(f"ğŸ“Œ ì¡°ê±´ C: w/o í‚¤ì›Œë“œ ì¶”ì¶œ ({max_samples}ê°œ ìƒ˜í”Œ)")
    
    if llm_service:
        result_no_kw = evaluate_without_keyword(samples, llm_service, max_samples)
        results['without_keyword'] = result_no_kw
        
        print(f"\n   Accuracy: {result_no_kw['accuracy']:.2%}")
        print(f"   ì •ë‹µ: {result_no_kw['correct']}/{result_no_kw['total']}")
    else:
        print("   âš ï¸ LLM ì„œë¹„ìŠ¤ ì—†ìŒ")
    
    # ===== ê²°ê³¼ ìš”ì•½ =====
    print("\n" + "=" * 70)
    print(f"ğŸ“Š SemEval 2014 {domain.capitalize()} ê²°ê³¼")
    print("=" * 70)
    
    print(f"\n{'ì¡°ê±´':<35} {'Accuracy':<15} {'ì •ë‹µ/ì „ì²´':<15}")
    print("-" * 65)
    print(f"{'A. ë³„ì  ê¸°ë°˜ (Baseline)':<35} {result_rating['accuracy']:.2%}")
    
    if 'full_model' in results:
        r = results['full_model']
        print(f"{'B. Full Model (LLM + í‚¤ì›Œë“œ)':<35} {r['accuracy']:.2%}{'':<6} {r['correct']}/{r['total']}")
    
    if 'without_keyword' in results:
        r = results['without_keyword']
        print(f"{'C. w/o í‚¤ì›Œë“œ ì¶”ì¶œ':<35} {r['accuracy']:.2%}{'':<6} {r['correct']}/{r['total']}")
    
    # ê°œì„ ìœ¨
    if 'full_model' in results:
        improvement = results['full_model']['accuracy'] - result_rating['accuracy']
        print(f"\nğŸ“ˆ Full Model ê°œì„ : {'+' if improvement >= 0 else ''}{improvement:.2%}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = project_root / 'experiments' / 'results' / f'semeval_{domain}.json'
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    save_results = {
        'dataset': f'SemEval 2014 {domain}',
        'total_samples': len(samples),
        'llm_samples': max_samples,
        **{k: v for k, v in results.items()}
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(save_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    return results


if __name__ == "__main__":
    run_semeval_experiment(domain='laptop', max_samples=1000)
